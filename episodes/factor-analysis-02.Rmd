---
title: 'Factor Analysis - CFA'
teaching: 10
exercises: 15
---

:::::::::::::::::::::::::::::::::::::: questions 

- What is a confirmatory factor analysis?
- How can I run CFA in R?
- How can I specify CFA models using lavaan?
- How can I interpret the results of a CFA and EFA in R?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Understand the difference between exploratory and confirmatory factor analysis
- Learn how to conduct confirmatory factor analysis in R
- Learn how to interpret the results of confirmatory factor analysis in R

::::::::::::::::::::::::::::::::::::::::::::::::

## Confirmatory Factor Analysis (CFA)
In the previous lesson, we approached the DASS as if we had no knowledge about the underlying factor structure. However, this is not really the case, since there is a specific theory proposing three interrelated factors. We can use confirmatory factor analysis to test whether this theory can adequately explain the item correlations found here.

Importantly, we not only need a theory of the number of factors, but also which items are supposed to load on which factor. This will be necessary when establishing our CFA model. Furthermore, we will test whether a one-factor model also is supported by the data and compare these two models, evaluating which one is better.

The main R package we will be using is `lavaan`. This package is widely used for Structural Equation Modeling (SEM) in R. CFA is a specific type of model in the general SEM framework. The lavaan homepage provides a great tutorial on CFA, as well (https://lavaan.ugent.be/tutorial/cfa.html). For a more detailed tutorial in SEM and mathematics behind CFA, please refer to the lavaan homepage or [this introduction](https://stats.oarc.ucla.edu/r/seminars/rsem/).

For now, let's walk through the steps of a CFA in our specific case. First of all, in order to do *confirmatory* factor analysis, we need to have a hypothesis to confirm. How many factors, which variables are supposed to span which factor?
Secondly, we need to investigate the correlation matrix for any issues (low correlations, correlations = 1). Then, we need to define our model properly using lavaan syntax. And finally, we can run our model and interpret our results.

## Model Specification
Let's start by defining the theory and the model syntax.

The DASS has 42 items and measures three scales: depression, anxiety, and stress. These scale are said to be intercorrelated, as they share common causes (genetic, environmental etc.). Furthermore, the DASS provides a detailed manual as to which items belong to which scales. The sequence of items and their respective scales is:

```{r}
library(dplyr)

dass_data <- read.csv("data/kaggle_dass/data.csv")

fa_data <- dass_data %>% 
  filter(testelapse < 600) %>% 
  select(starts_with("Q")& ends_with("A"))

item_scales_dass <- c(
  "S", "A", "D", "A", "D", "S", "A", 
  "S", "A", "D", "S", "S", "D", "S",
  "A", "D", "D", "S", "A", "A", "D",
  "S", "A", "D", "A", "D", "S", "A", 
  "S", "A", "D", "S", "S", "D", "S",
  "A", "D", "D", "S", "A", "A", "D"
)

item_scales_dass_data <- data.frame(
  item_nr = 1:42,
  scale = item_scales_dass
)

item_scales_dass_data
```

::: callout
## Scales vs. Factors

There is an important difference between *scales* and *factors*. Again, factors represent mathematical constructs discovered or constructed during factor analysis. Scales represent a collection of items bound together by the authors of the test and said to measure a certain construct. These two words cannot be used synonymously. Scales refer to item collections whereas factors refer to solutions of factor analysis. A scale can build a factor, in fact its items should span a common factor if the scale is constructed well. But a scale is not the same thing as a factor.
:::

Now, in order to define our model we need to keep the distinction between manifest and latent variables in mind. Our manifest, *measured* variables are the items. The factors are the latent variable. We can also define which manifest variables contribute to which latent variables. In lavaan, this can be specified in the model code using a special syntax.

```{r}
library(lavaan)

model_cfa_3facs <- c(
  "
  # Model Syntax
  
  # =~ is like in linear regression
  # left hand is latent factor, right hand manifest variables contributing
  
  # Define which items load on which factor
  depression =~ Q3A + Q5A + Q10A + Q13A + Q16A + Q17A + Q21A + Q24A + Q26A + Q31A + Q34A + Q37A + Q38A + Q42A
  
  anxiety =~ Q2A + Q4A + Q7A + Q9A + Q15A + Q19A + Q20A + Q23A + Q25A + Q28A + Q30A + Q36A + Q40A + Q41A
  
  stress =~ Q1A + Q6A + Q8A + Q11A + Q12A + Q14A + Q18A + Q22A + Q27A + Q29A + Q32A + Q33A + Q35A + Q39A
  
  # Define correlations between factors using ~~
  depression ~~ anxiety
  depression ~~ stress
  anxiety ~~ stress
  "
)
```

Now, we can fit this model using `cfa()`.

```{r}
fit_cfa_3facs <- cfa(
  model = model_cfa_3facs, 
  data = fa_data,
  )
```

To inspect the model results, we use `summary()`.

```{r}
summary(fit_cfa_3facs, fit.measures = TRUE, standardize = TRUE)
```

Now, this output contains several key pieces of information: 

- fit statistics for the model
- information on item loadings on the latent variables (factors)
- information on the correlations between the latent variables
- information on the variance of latent and manifest variables

Let's walk through the output one-by-one.

## Fit measures

The model fit is evaluated using three key criteria.

The first is the Chi^2 statistic. This evaluates whether the model adequately reproduces the covariance matrix in the real data or whether there is some deviation. Significant results indicate that the model does not reproduce the covariance matrix. However, this is largely dependent on sample size and not informative in practice. The CFA model is always a reduction of the underlying covariance matrix. That is the whole point of a model. Therefore, do not interpret the chi-square test and its associated p-value. Nonetheless, it is customary to report this information when presenting the model fit.

The following two fit statistics are independent of sample size and widely accepted in practice.

The CFI (Comparative Fit Index) is an indicator of fit. It ranges between 0 and 1 with 0 meaning horrible fit and 1 reflecting perfect fit. Values above 0.8 are acceptable and CFI > 0.9 is considered good. This index is independent of sample size and robust against violations of some assumptions CFA makes. It is however dependent on the number of free parameters as it does not include a penalty for additional parameters. Therefore, models with more parameter will generally have better CFI than parsimonious models with fewer parameters.

The RMSEA (Root Mean Squared Error Approximation) is an indicator of misfit that includes a penalty for the number of parameters. The higher the RMSEA, the worse. Generally, RMSEA < 0.05 is considered good and RMSEA < 0.08 acceptable. Models with RMSEA > 0.10 should not be interpreted. This fit statistic indicates whether the model fits the data well while controlling for the number of parameters specified.

It is customary to report all three of these measures when reporting the results of a CFA. Similarly, you should also interpret both the CFI and RMSEA, as bad fits in both statistics can indicate issues in the model specification. The above model would be reported as follows. 

The model with three correlated factors showed acceptable fit to the data $\chi^2(816) = 107309.08, p < 0.001, CFI = 0.90, RMSEA = 0.06, 90% CI = [0.06; 0.06]$.

note that for model comparison, sometimes the information criteria are used (AIC/BIC). More on this later RMSEA can also be used.

## Loadings on latent variables

Standardized vs. no

What loading is "good"

Same thing with marker items

::: callout
## Key Difference between EFA and CFA

There is one fundamental difference between a CFA and an 

Key difference between CFA and EFA are item loadings. CFA only allows items to load on one factor, EFA on all
:::

## Correlations between latent variables
Notice how this is different than in the EFA!
Depending on your theory, you might be looking at different things

## Variances
This may be useful in some cases if you check whether there is a certain latent factor. For example, when testing for another overarching factor

## Model Comparison

::: challenge 

## Challenge 1

Read in data and answer the question: do we have a hypothesis as to how the structure should be?

:::

::: challenge 

## Challenge 2

Run an EFA, how many factors would you extract using the Keiser criterion, how many using the scree-plot

:::

::: challenge 

## Challenge 3

Run a CFA using a one-factor model for all DASS related items. How well does it fit

:::

::: challenge 

## Challenge 4

Run a second CFA using a three factor model with correlated factors. How well does it fit?
:::

::: challenge 

## Challenge 5

Compare the models from challenge 3 and challenge 4, which one fits better?

:::

::::::::::::::::::::::::::::::::::::: keypoints 

- Something

::::::::::::::::::::::::::::::::::::::::::::::::

